{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avacado Project\n",
    "Problem Statement:\n",
    "Avocado is a fruit consumed by people heavily in the United States. \n",
    "\n",
    "Content\n",
    "This data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. \n",
    "\n",
    "The table represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. \n",
    "\n",
    "Starting in 2013, the table reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. \n",
    "\n",
    "The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
    "\n",
    "Some relevant columns in the dataset:\n",
    "\n",
    "Date - The date of the observation\n",
    "AveragePrice - the average price of a single avocado\n",
    "type - conventional or organic\n",
    "year - the year\n",
    "Region - the city or region of the observation\n",
    "Total Volume - Total number of avocados sold\n",
    "4046 - Total number of avocados with PLU 4046 sold\n",
    "4225 - Total number of avocados with PLU 4225 sold\n",
    "4770 - Total number of avocados with PLU 4770 sold\n",
    "\n",
    "\n",
    "\n",
    "Inspiration /Label \n",
    "\n",
    "Task is to make a mode that can consider the data provided and predict the Average Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importing the all library which is required for EDA, visualization, prediction . \n",
    "The reason of doing this is that it become easier to use all the import statement at one go \n",
    "rather than importing the statement again at each point. \n",
    "We could find all the import statement at one place without searching it on whole notebook and can update also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loadig the data into the dataframe and performing all the basic fuctionalities required like shape ,datatypes ,checking sum of null values,This helps us in getting an overall idea about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As seen in data set there is  index column which does not play any important role for prediction in the price of avocado, so I am dropping that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are more than half null values in the dataset . Since both rows and columns are null,we can just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to check the shape of dataset after dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are a t otal of 1517 rows and 13 colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datatypes are mostly float ,with couple of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are only 2 years under study.We obtained this by chekcing unique command.\n",
    "Describe command gives us an idea about the standard deviation,mean ,skewness ,outliers ,min and max value and percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to find out the correlation between the features and we found out that features are correlated with each other apart from average price.We will convert the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Visualization:\n",
    "In this portion we can plot different graph using different columns to visualize the data using matplotlib and seaborn library.\n",
    "We use different graph include:\n",
    "Bar plot\n",
    "Count plot\n",
    "Line plot\n",
    "Box plot\n",
    "\n",
    "from above analysis we found that 2016 is the year which average price is higher .\n",
    "November and February -the average price of avocads are higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label Encoding:\n",
    "Sklearn provides a very efficient tool for encoding the levels of categorical features into numeric values. \n",
    "Label Encoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. \n",
    "If a label repeats it assigns the same value to as assigned earlier\n",
    "Convert Region and Type into numeric value by using encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers:\n",
    "An outlier is a data point in a data set that is distant from all other observations. \n",
    "A data point that lies outside the overall distribution of the data set.\n",
    "\n",
    "\n",
    "From box plot we can clearly see that there are number of black dots in most of the column which are referring \n",
    "to the outliers, so it means most of the data are outside the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since we detect the outliers,thenext step is to remove them. I am using ZSCORE .So, I first find the zscore value \n",
    "and decided to make one threshold value as 3 which is standard of industry recommend value \n",
    "and remove all the outliers which zscore value is greater than 3.\n",
    "\n",
    "After, removing the outlier’s final there are 1436 and 13 column presents in the data set.The percentage dataloss is at \n",
    "5% which we can afford to loose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitting the data into target variable and input features.\n",
    "Also, I am using the standard scaling method on x variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I have written a function tofind out the random state which maximum accuracy is coming and I got the result as 199.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writing a common function to get the r2 score ,\n",
    "mean absolute error,root mean square error and passing the functions one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods used for anlysis are\n",
    "DecisionTreeRegressor\n",
    "LinearRegression()\n",
    "AdaBoostRegressor()\n",
    "GradientBoostingRegressor()\n",
    "KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concluding Remarks\n",
    "\n",
    "Parameter tuning for Gradient as it has the highest r2 score.GridSearchCV has the highest r2 score \n",
    "and we checked the n_estinator params and got the value as loss and at 99\n",
    "\n",
    "Creating the final model withthe given parameters and saving the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
